\section{Réflexion}

cette section décrit quelques réflexions provoquées par ce cours.

\subsection{Conception}

Actuellement, on assiste à une véritable explosion du marché des systèmes temps
réel.Toutes les nouvelles cartes sont équipées de coupleurs Ethernet pour les
accès Internet. L'utilisation des systèmes temps réel embarqués avec un accès
réseau est maintenant très répandue dans plusieurs domaines, notamment les
télécommunications, les automatismes industriels, l'industrie de l’automobile,
etc. La taille de la partie logicielle de ces dispositifs est de plus en plus
importante. Elle représente une contrainte très significative sur les ressources
qui sont, par essence, comptées.

Devant cette évolution vertigineuse, l'ingénierie, sous ses différents domaines,
paraît l'élément clé du processus de conception. La collaboration entre les
différentes disciplines, assurée par une approche ingénierie système, a permis
de mettre au point des solutions aux demandes industrielles les plus complexes.

La solution générée peut répondre aux cahier des charges certes, mais est elle
efficace ?

\subsection{Front de Pareto}

D'abord, pour la conception d'un système complexe qui met en jeu plusieurs
disciplines, l'optimisation est multi-objectifs et l'optimum global n'existe
pas. L'espace des possibles est divisé en deux familles de solutions :
\begin{itemize}
\item les solutions uniformément améliorables : une solution de cette famille
peut être améliorée suivant un critère sans qu'aucun autre critère ne soit
détérioré.
\item les solutions non uniformément améliorables : une solution de cette
famille ne peut être améliorée suivant un ou plusieurs critères sans détériorer
au moins un des autres critères. Ces solution forment la frontière de Pareto.
\end{itemize}

Pour des produits complexes, il est possible que les processus de conception ne
permettent pas d'atteindre un optimum (de Pareto). La concurrence et l'évolution
des technologies, par exemple dans le domaine des semi conducteurs, forcera les
industries à transiter vers une nouvelle ère de conception. Cette ère sera
caractérisée par un certain degrés d'automatisation du processus de conception
qui limitera les biais apportés par le concepteur humain.

\subsection{Outils de CAO}

Pour simuler la fatigue au niveau de la structure mécanique d'un drone un
ingénieur peut utiliser des outils tels que CATIA. Pour analyser
temporellement un modèle de réseau embarqué son coéquipier utilisera VNA MENTOR
GRAPHICS. Pour modéliser un circuit électronique, il existe aussi de très
nombreux outils.

On peut alors penser la conception comme un ensemble de contraintes définies au
moyen de l'ingénierie système. Les contraintes sont ensuite réparties en
domaines. Pour chaque domaine, la conception s'effectue avec des outils
spécifiques. Dans ce scénario et pour chaque discipline l'ingénieur utilise une
interface graphique ou un environnement pour décrire son modèle et le simuler.
L'optimisation reste locale à ce modèle. Jusqu'à quel point peut-on se contenter
de la somme des optimisations locales ?

L’auto-génération de code ou l’auto-conception à partir des spécifications
fixées dans le cahier des charges est donc importante. On peut distinguer deux
approches : d'un côté, un algorithme qui part d’une architecture réalisée
arbitrairement et, à chaque itération réalise les modifications sur
l’architecture en minimisant un ou plusieurs coûts donnés, de l'autre côté, un
algorithme qui transforme un ensemble de contraintes en une architecture qui
les respecte.

~

Illustrons ceci avec un exemple. Une entreprise veut concevoir un produit qui
contiendra un réseau embarqué. Ses ingénieurs disposent d'une licence VNA MENTOR
GRAPHICS. L'architecture de communication est suffisamment complexe pour qu'une
architecture choisie arbitrairement soit ne soit \textit{a priori} ni optimale
ni même respectueuse des contraintes.

Pour réaliser un outil de génération d’architecture, il faudrait utiliser
l’interfacce import/export de VNA Mentor Graphics qui supporte FIBEX XML.
L’utilisation FIBEX parait adéquate vu qu’il s’agit d’un format XML
utilisé pour décrire des systèmes complexes de communication orientée messages.
La réalisation de l’outil revient principalement à être capable de traduire des
spécifications en entrée sous forme d’une description XML en sortie et de
coupler cette étape à l’outil VNA. Les différentes étapes sont :
\begin{enumerate}
\item Décrire les modules demandés et une architecture de communication, qui
répond aux spécifications mais non optimale, sous VNA Mentor Graphics.
\item Exporter la description de l’architecture sous format FIBEX XML.
\item Réaliser l’étape 0 de l’algorithme d’optimisation sur la description XML.
\item Importer l’architecture par l’outil VNA Mentor Graphics.
\item Simuler l’architecture et obtenir des résultats de simulation.
\item Si résultats de simulation ne répondent pas aux objectifs de minimisation,
reprendre à l’étape 3.
\end{enumerate}

Les principales difficultés qui émergent de cet exemple sont 1) la description
initiale d'une architecture, cette étape pourrait aussi être automatisée, 2) la
convergence de l'algorithme.

~

\`A partir de cette réflexion une nouvelle vision se dégage : celle d'un
processus de conception où l'intervention de l'ingénieur est limitée par
l'automatisation de modèlisation/simulation/optimisation pour les domaines
respectifs.

Jusqu'à présent, l'innovation réside dans l'automatisation/optimisation de la
conception pour chacune des disciplines séparément. En effet, l'optimisation
pour chaque discipline pourrait prendre des contraintes qui découlent d'autres
processus de conception parallèles, mais l'amélioration ne peut porter que sur
les variables du processus en question.

\subsection{Simulation multiphysique}

Dans ce sens, il devient légitime de penser à une optimisation multi-objectifs
et donc une simulation multiphysique. Cela est il possible?

Prenons le cas MATLAB/SIMULINK : propose une interface avec DYMOLA et avec du
code C/C++, VHDL ou SystemC. DYMOLA admet une interface avec CATIA\ldots

En somme, la simulation d'un système complet nécessite de centraliser l'accès
aux différents modèles à travers un ensemble de plug-ins. Il devient ensuite
possible d'automatiser la conception d'un système complexe en ne disposant que
du cahier des charges à l'instant de départ.

\subsection{Mur de la simulation et extraction de Modèle}

Néanmoins, cette approche d'intégration brutale n'est pas faisable. On ne peut
pas espérer simuler en un temps raisonnable ne serait ce qu'un millier de
transistors dont on aurait définit la géométrie par éléments finis et en
résolvant de manière numérique les équations différentielles thermiques et
quantiques. Plus la simulation est grande en taille, plus il faut un modèle
haut-niveau et rapide à simuler.

Il faut donc être capable d'extraire du plus bas niveau des paramètres pour les
modèles du niveau supérieur et ainsi récursivement. Cela entraîne une
déperdition d'information à mesure qu'on la synthétise, surtout en ce qui
concerne les interdépendance des paramètres. Dans le cas d'un inverseur CMOS
par exemple, si on considère dans une technologie donnée seulement la dépendance
entre la géométrie des transistors, la température et le temps de propagation,
on n'oublie la dépendance avec les tensions d'alimentation. L'analyse CEM
perd alors en précision. Néanmoins, dans cet exemple, on peut faire des
encadrements
assez précis si on limite les perturbations autorisées sur les alimentations.
Ainsi, de manière générale, la qualité d'un modèle dépend de la précision de la
prédiction des paramètres d'entrée en fonction des paramètres de sorties mais
aussi de la définition du domaine de validité du modèle.

Même si ce paradigme d'extraction de modèle permet probablement de simuler avec
précision un système, son application à l'auto-conception est mise en difficulté
par la nécessité de construire ces modèles. On peut peut-être espérer
l'émergence de standards pour ces modèles à la manière du standard IBIS pour les
buffers d'entrée/sortie et la mise à disposition des modèles par les
fournisseurs d'IPs aux concepteurs de systèmes.

\subsection{Atteindre etrepousser le front de Pareto}

Supposons que le problème de l'auto-conception puisse se poser comme un problème
d'agencement d'un ensemble d'IPs.

Dans notre méthodologie comme dans de nombreux flots de conception présentés en
cours, la conception se fait par itérations : si le modèle ne correspond pas aux
attentes alors on l'ajuste. On est donc confronté à toutes les problématiques
d'exploration et d'optimisation en grande dimension et il faut avoir recourt aux
techniques appropriées. De manière tout à fait profane, il nous semble que le
point clé est d'arriver à extrapoler les interdépendances des paramètres pour
déterminer la direction d'un optimum et espérer une convergence vers un point
intéressant. Le problème est que les interdépendances sont peut-être très
chaotiques.

Même si le concepteur peut espérer concevoir un système optimal au sens de
Pareto, il n'a pas de mécanismes à sa disposition pour repousser la frontière.
En effet, le front de Pareto est délimité par les IPs auxquelles il a accès.
Il peut espérer concevoir un système optimal au sens de Pareto mais n'a pas de
mécanismes à sa disposition pour repousser la frontière. Il n'a peut-être pas la
compréhension suffisante des modèles pour discuter avec les fournisseurs d'un
nouveau composant.

On voit apparaître la nécessité d'une dynamique entre les entités que l'on a
nommées << concepteurs >> et << fournisseurs >> pour permettre l'innovation.
Il s'agit soit d'élargir les paramètres de conception (nouvelle technologie de
circuits intégrés par exemple) soit de rajouter une dimension (circuits 3D par
exemple). Il semble que les concepteurs puissent communiquer leur compréhension
de la frontière, leurs besoins pour que les fournisseurs puissent chercher des
axes de recherche. Malheureusement, comme aucun modèle de modèle n'est parfait,
le format de modélisation de sera peut-être pas à même d'inclure ces nouveaux
paramètres. Il faudra donc du temps et de l'argent pour que des nouvelles
technologies soient complétement intégrées dans les process d'auto-conception.
On peut donc supposer que les grands groupes qui se seront établis ne seront pas
forcément moteurs dans le développement mais plutôt des intégrateurs
d'innovations provenant de plus petites structures.

\subsection{Bilan}

En résumé, la complexité des problèmes fait apparaître des frontières de Pareto.
L'optimisation requiert un décloisonnement des domaines et l'automatisation du
processus de conception. Une conception automatique et itérative nécessite de la
simulation multiphysique qui doit s'appuyer sur des modèles synthétiques.
L'utilisateur des modèles dépend de ceux les maîtrisant. Une interaction est
nécessaire pour une synergie des acteurs. Cette synergie offre des perspectives
de dynamisme.

Il s'agit d'une présentation assez simplifiée qui correspond aux grandes lignes
que nous avons pu distinguer dans la vision offerte jusqu'à présent par nos
cours et nos expériences


